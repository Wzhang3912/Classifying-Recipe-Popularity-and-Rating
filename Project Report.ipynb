{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0180184d",
   "metadata": {},
   "source": [
    "# Classifying Recipe Popularity and Rating with Imbalanced Data \n",
    "\n",
    "By Weijie Zhang (wez042@ucsd.edu)\n",
    "\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "This data science project aims to explore some of the characteristics of popular and highly rated food recipes. The data comes from food.com and was originally scraped and used by the authors of this recommender system paper.\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Nowadays, cooking and sharing recipes online has become widely popular and a significant part of many people’s lives. With the convenience of online recipe platforms like food.com, millions of users benefit from having access to a vast collection of recipes in a variety of culinary. Among all the recipe options available, however, some recipes stand out as both particularly popular and highly rated. \n",
    "\n",
    "In this project, we dive into an extensive collection of food recipe data sourced from food.com. The first dataset we are going to use contains food recipes from 2008 to 2018 on food.com, with detailed information about a recipe such as ingredients, preparation times, nutritional values, etc. This dataset contains 83782 observations, meaning that there are 83782 unique recipes in total in our data. The second dataset contains information about users’ review comments and ratings submitted for the recipes in the first dataset. This dataset contains 731927 number of reviews in total submitted by users. Some recipes have lots of reviews while others might have fewer. \n",
    "\n",
    "<br>\n",
    "\n",
    "Let’s take a closer look at the information we have. Below is the description of some of the relevant columns in the first dataset:\n",
    "\n",
    "\n",
    "|      Column      |       Description         |\n",
    "|------------------|       ------------       |\n",
    "|   `name`         |       Recipe name         |\n",
    "|   `minutes`      |       Minutes to prepare food |\n",
    "|   `submitted`    |       Date recipe was uploaded |\n",
    "|   `tags`         |       Food.com tags for recipe |\n",
    "|   `nutrition`    |      Nutrition information for the recipe, <br> including calories, total fat, sugar, sodium <br>protein, saturated fat, and carbohydrates |\n",
    "|   `steps` |  Steps to make the food by the recipe steps    |\n",
    "|   `description` |   Recipe description    |\n",
    "|   `ingredients` |      List of ingredients for recipe  |\n",
    "\n",
    "\n",
    "Let’s take a look at the second dataset:\n",
    "\n",
    "|      Column      |       Description         |\n",
    "|-----------------|       ------------          |\n",
    "|   `date`         |       Date reviews was submitted       |\n",
    "|   `rating`       |       Rating given by the user         |\n",
    "|   `review`       |       Review comment given by the user |\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "By combining these two comprehensive datasets, we aim to answer the following research questions throughout the project: **What are some of the characteristics of the recipe that are both popular and highly rated? What category does a recipe fall into based on its predicted popularity level and average rating score?**\n",
    "\n",
    "Category 1: Low review count, low average rating\n",
    "\n",
    "Category 2: Low review count, high average rating\n",
    "\n",
    "Category 3: High review count, low average rating\n",
    "\n",
    "Category 4: High review count, high average rating\n",
    "\n",
    "Note: we define the threshold for high and low review counts as **10** reviews count and the threshold for high and low average rating scores as **4.8**\n",
    "\n",
    "<br>\n",
    "\n",
    "Through a rigorous analysis of these datasets, we will explore univariate and bivariate relationships between key variables, investigate missing values in the dataset, conduct hypothesis testing to answer the question of interest, build and evaluate a classifier model, and ultimately conduct fairness analysis for the final model. This project will provide valuable insights into understanding the key characteristics of certain recipes that stand out as exceptional and loved by many. \n",
    "\n",
    "---\n",
    "\n",
    "### Univariate Analysis\n",
    "\n",
    "In the univariate analysis, we will look at the distribution of the calories and the distribution of the rating in food recipes in the dataset. \n",
    "\n",
    "<br>\n",
    "\n",
    "#### Distribution of Calories\n",
    "\n",
    "<iframe src=\"figures/uni_var_1.html\" width=800 height=600 frameBorder=0></iframe>\n",
    "\n",
    "(This is a histogram of the distribution of the calories of food recipes in the dataset.)\n",
    "\n",
    "We only included 98% of the data values from the `calories` column for better visualization. From looking at the histogram, we see that most of the calorie values range from 0 to 1000, with fewer data values towards the right side of the plot. The distribution shows right-skewness, indicating that a significant proportion of food recipes fall within a typical calorie range, with some recipes featuring higher calorie counts.\n",
    "\n",
    "---\n",
    "\n",
    "#### Distribution of Rating\n",
    "\n",
    "<iframe src=\"figures/uni_var_2.html\" width=800 height=600 frameBorder=0></iframe>\n",
    "\n",
    "(This is a histogram of the distribution of the rating of food recipes in the dataset.)\n",
    "\n",
    "As we look at the histogram, the majority of recipes have an average rating above 3.0, with only a very small minority falling below this threshold. If we want to develop a classification model to distinguish recipes based on good and bad ratings, we must address the issue of **imbalanced data**. This is something we should consider when building our final classification model. \n",
    "\n",
    "---\n",
    "\n",
    "### Bivariate Analysis\n",
    "\n",
    "For bivariate analysis, we will look at the relationship between sugar percent daily value and rating range, as well as, calories percent daily value and the number of ingredients in food recipes in the dataset. \n",
    "\n",
    "<br>\n",
    "\n",
    "#### Sugar vs. Rating\n",
    "\n",
    "<iframe src=\"figures/bi_var_1.html\" width=800 height=600 frameBorder=0></iframe>\n",
    "\n",
    "(This is a box plot featuring the relationship between rating and sugar percent daily value in food recipes in the dataset.)\n",
    "\n",
    "In crafting this box plot, we only incorporated 95% of the data values from the `sugar` column to exclude extreme values and grouped ratings into bins for better visualization. As we look into the box plot, we note a slight upward trend in the sugar daily level for the recipes as the average rating decreases, particularly in the third quarter and the upper fence. This observation suggests a potential **negative correlation between the sugar content and the average rating in recipes**, indicating that low sugar content might tend to receive higher ratings, and vice versa. \n",
    "\n",
    "---\n",
    "\n",
    "#### Calories vs. Number of Ingredients\n",
    "\n",
    "<iframe src=\"figures/bi_var_2.html\" width=800 height=600 frameBorder=0></iframe>\n",
    "\n",
    "(This is a scatter plot featuring the relationship between calorie percent daily value and the number of ingredients in food recipes in the dataset.)\n",
    "\n",
    "\n",
    "In making this visualization, we only included 99% of the data value for the `calories` column to exclude extreme values, grouped ratings into bins, and introduced random noise to the `n_ingredients` column, which represents a discrete variable, for better visualization. From examining the scatter plot, we see that there exists a **week positive correlation between the calorie daily level and the number of ingredients in recipes**. This finding aligns with our expectation that using a variety of ingredients in a recipe would likely result in higher calorie content in food.\n",
    "\n",
    "---\n",
    "\n",
    "### Data Aggregation\n",
    "\n",
    "So far, we only looked at the overall distribution of one variable and the bivariate distribution of two variables. For a better understanding of our data, we construct the following pivot table that shows the aggregated distribution of quantitative variables by mean, conditional on the number of ingredients. As we examine each column of the pivot table from top to bottom, we can observe a consistently increasing trend in various quantitative variables such as `calories`, `carbohydrates`, `minutes`, `n_steps`, `protein`, `saturated_fat`, `sodium`, and `total_fat`, as the number of ingredients increases in food recipes. This suggests that `n_ingredients` might have a positive correlation with these quantitative variables. \n",
    "\n",
    "\n",
    "The pivot table is shown below (number is rounded to 3 decimal places)\n",
    "\n",
    "|   n_ingredients |   calories |   carbohydrates |   minutes |   n_steps |   protein |   saturated_fat |   sodium |   sugar |   total_fat |\n",
    "|----------------|-----------|----------------|----------|----------|----------|----------------|---------|--------|------------|\n",
    "|               1 |    288.77  |          10.2   |    47.6   |     7.3   |    10.8   |          25     |   12.9   | 100.1   |      21.8   |\n",
    "|               2 |    238.312 |           8.728 |    56.131 |     5.711 |    13.18  |          18.211 |    7.699 |  62.322 |      15.27  |\n",
    "|               3 |    233.924 |           8.413 |    42.754 |     5.485 |    13.873 |          19.791 |   10.844 |  55.622 |      15.406 |\n",
    "|               4 |    263.534 |           9.084 |    40.107 |     6.135 |    16.943 |          24.627 |   12.858 |  60.599 |      18.164 |\n",
    "|               5 |    282.587 |           9.288 |    49.087 |     7.104 |    19.914 |          26.891 |   15.712 |  54.497 |      20.726 |\n",
    "|               6 |    300.474 |           9.7   |    51.857 |     7.716 |    22.256 |          28.382 |   17.644 |  52.118 |      22.189 |\n",
    "|               7 |    326.27  |           9.87  |    56.538 |     8.413 |    26.021 |          31.473 |   19.369 |  46.82  |      25.221 |\n",
    "|               8 |    339.984 |          10.234 |    58.947 |     9.214 |    28.029 |          32.541 |   20.882 |  45.587 |      26.283 |\n",
    "|               9 |    353.065 |          10.757 |    63.083 |     9.853 |    30.069 |          32.736 |   21.893 |  44.457 |      26.781 |\n",
    "|              10 |    366.854 |          10.979 |    67.319 |    10.709 |    32.143 |          34.049 |   22.93  |  45.52  |      27.997 |\n",
    "|              11 |    386.087 |          11.631 |    69.016 |    11.21  |    34.144 |          35.42  |   24.741 |  47.409 |      29.362 |\n",
    "|              12 |    402.617 |          12.329 |    72.786 |    11.912 |    35.873 |          36.136 |   25.677 |  49.844 |      30.143 |\n",
    "|              13 |    419.473 |          12.785 |    75.468 |    12.585 |    37.439 |          37.407 |   26.52  |  51.117 |      31.519 |\n",
    "|              14 |    437.62  |          13.139 |    81.278 |    13.326 |    39.786 |          39.264 |   28.078 |  51.233 |      33.134 |\n",
    "|              15 |    461.443 |          13.811 |    85.581 |    14.249 |    43.113 |          40.383 |   30.858 |  49.922 |      34.574 |\n",
    "|              16 |    486.545 |          14.736 |    82.214 |    14.924 |    45.352 |          42.702 |   33.729 |  51.809 |      36.217 |\n",
    "|              17 |    493.79  |          14.64  |    92.491 |    15.594 |    46.517 |          43.666 |   33.938 |  54.435 |      37.395 |\n",
    "|              18 |    532.985 |          15.415 |    98.909 |    16.39  |    50.407 |          47.228 |   36.299 |  54.46  |      40.908 |\n",
    "|              19 |    526.863 |          15.781 |    97.143 |    16.714 |    49.113 |          44.232 |   37.453 |  53.226 |      39.722 |\n",
    "|              20 |    549.916 |          15.232 |    99.352 |    17.654 |    52.645 |          47.735 |   40.738 |  47.756 |      43.358 |\n",
    "|              21 |    554.031 |          15.832 |   116.699 |    17.878 |    54.418 |          49.806 |   42.719 |  52.015 |      42.76  |\n",
    "|              22 |    586.774 |          17.954 |   131.708 |    19.685 |    56.208 |          49.023 |   41.831 |  64.338 |      43.069 |\n",
    "|              23 |    582.013 |          17.024 |   141.25  |    19.321 |    53.048 |          50.679 |   52.095 |  63.988 |      45.619 |\n",
    "|              24 |    628.248 |          17.029 |   130.754 |    19.884 |    63.217 |          46.072 |   44.623 |  49.623 |      49.681 |\n",
    "|              25 |    669.144 |          18.969 |   109.781 |    21.594 |    66.781 |          60.188 |   63.969 |  66.594 |      52     |\n",
    "|              26 |    531.532 |          17     |   104.12  |    18.36  |    47.28  |          44.72  |   42.96  |  52.12  |      39.48  |\n",
    "|              27 |    724.389 |          20.333 |   153.167 |    21.167 |    67.667 |          75.556 |   44.111 |  64.389 |      58     |\n",
    "|              28 |    537.787 |          16.133 |   111     |    28.333 |    61.467 |          38.467 |   52.733 |  54.533 |      37.2   |\n",
    "|              29 |    886.3   |          34     |    86.222 |    21.222 |    78     |          72.333 |   59.444 | 141.778 |      60     |\n",
    "|              30 |    631.656 |          17.333 |   116.667 |    20     |    55.222 |          62.222 |   51     |  41.222 |      53.222 |\n",
    "|              31 |    502.05  |          14.667 |   197.5   |    22.333 |    47.667 |          45     |   46.5   |  44.667 |      39.833 |\n",
    "|              32 |    697.35  |          18.5   |    55     |    34     |    66     |          87.5   |   53     |  30.5   |      58.5   |\n",
    "|              33 |    338.2   |          14     |    35     |     6     |     8     |          12     |   16     |  18     |      25     |\n",
    "\n",
    "<br>\n",
    "\n",
    "To get a visual representation of the above data, we have converted the pivot table to a series of line plots as below. \n",
    "\n",
    "<iframe src=\"figures/agg.html\" width=800 height=600 frameBorder=0></iframe>\n",
    "\n",
    "As we can see from the line plots above which illustrates the relationship between the number of ingredients and various other quantitative columns such as `calories`, `carbohydrates`, `minutes`, `n_steps`, `protein`, `saturated_fat`, `sodium`, and `total_fat`, we can observe clear and consistent increasing trends across all plots, aligns with our observation from the pivot table. These findings suggest a positive correlation between the number of ingredients and these quantitative variables, indicating that, on average, having more ingredients included in a food recipe corresponds to higher nutritional values, more preparation time, and a greater number of steps. \n",
    "\n",
    "---\n",
    "\n",
    "## Assessment of Missingness\n",
    "\n",
    "\n",
    "### Not Missing At Random (NMAR) Analysis \n",
    "\n",
    "We believe that the missing values in the `rating` column are not Not Missing at Random (**NMAR**), meaning that the chance of a value being missing **depends on the actual missing values themselves.** Updon browersing from the food.com website, we found that there are certain reviews given by users without ratings in the comment section of a recipe (showing in the image below). This is the case in which users opt to provide a review comment, without giving a rating score to the recipe. Therefore, during the data generation process, this result in certain recipes having missing rating score values.\n",
    "\n",
    "<img src=\"figures/Image_1.png\" alt=\"NMAR example\">\n",
    "\n",
    "---\n",
    "\n",
    "### Missingness Dependency Analysis\n",
    "\n",
    "Besides the `rating` column, we also found missing values in `description` column. We hypothesize that the missingness of `description` is **Missing at Random (MAR)**, meaning that the chance of a value being missing **depends on some other columns**. We will investigate the missing dependency of `description` using permutation test. \n",
    "\n",
    "We hypothesize that the missingness of `description` column depends on the `sugar` column, meaning that there is some systemic difference between the distribution of sugar for those recipes missing the description and those that do not. \n",
    "\n",
    "<br>\n",
    "\n",
    "Set up:\n",
    "\n",
    "**Does the missingness of `description` depend on the `sugar` column?**\n",
    "\n",
    "**Null Hypothesis**: the missingness of `description` does not depend on `sugar`. \n",
    "\n",
    "**Alternative Hypothesis**: the missingness of `description` does depend on `sugar`.\n",
    "\n",
    "**Significance Level**: 0.05\n",
    "\n",
    "\n",
    "\n",
    "Let's look at the distribution of `sugar`, conditional on the missingness of `description` column. \n",
    "\n",
    "<iframe src=\"figures/miss_dist_1.html\" width=800 height=600 frameBorder=0></iframe>\n",
    "\n",
    "Based on the plot above, the orange line represents the distribution of sugar when `description` is not missing, while the blue line represents the distribution of sugar when `description` is missing. We can see that these two distributions of sugar, conditional on the missingness of `description`, looks quite difference. To quantify this difference, we decide to use Kolmogorov-Smirnov (K-S) statistic instead of the absolute difference of means. Therefore, we will use **K-S statistics** as our test statistics for the permutation test.\n",
    "\n",
    "\n",
    "\n",
    "We shuffle the missingness of description 1000 times and get 1000 simulated K-S test statistics of `sugar` column during the permutation test. The permutation result is shown as below. \n",
    "\n",
    "<iframe src=\"figures/miss_perm_1.html\" width=800 height=600 frameBorder=0></iframe>\n",
    "\n",
    "From the plot above and result of the permutation test, the 0.05% significance level of the simulated statistics is 0.16 and our observed test statistics 0.18, which is greater that the values of the significance level. The p-value is 0.023, which is less than the significance level of 0.05. So we **reject the null hypothesis.**\n",
    "\n",
    "Thus, we conclude that **the missingness of `descrpition` likely depends on the `sugar` column.**\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "We hypothesize that the missingness of `description` column does not depend on the `calorie` column, meaning that the distribution of calorie when `description` is missing and the distribution of calorie when `description` is not missing are alike, any difference is due to random chance. \n",
    "\n",
    "\n",
    "\n",
    "Set up:\n",
    "\n",
    "**Does the missingness of `description` depend on `minutes` column?**\n",
    "\n",
    "**Null Hypothesis**: the missingness of `description` does not depend on `minutes`. \n",
    "\n",
    "**Alternative Hypothesis**: the missingness of `description` does depend on `minutes`.\n",
    "\n",
    "**Significance Level**: 0.05\n",
    "\n",
    "\n",
    "\n",
    "Let's look at the distribution of `minutes`, conditional on the missingness of `description` column. \n",
    "\n",
    "<iframe src=\"figures/miss_dist_2.html\" width=800 height=600 frameBorder=0></iframe>\n",
    "\n",
    "Based on the plot above, the orange line illustrates the distribution of minutes when `description` is not missing, while the blue line illustrates the distribution of minutes when `description` is missing. Desipte both distribution have similar shape, they are centered in smiliar location. Using difference in mean may not effectively capture this difference in distribution. So we decide to use Kolmogorov-Smirnov (K-S) statistic as our test statistics for the permutation test.\n",
    "\n",
    "\n",
    "\n",
    "We shuffle the missingness of description 1000 times and get 1000 simulated K-S test statistics of `minutes` column during the permutation test. The permutation result is shown as below. \n",
    "\n",
    "<iframe src=\"figures/miss_perm_2.html\" width=800 height=600 frameBorder=0></iframe>\n",
    "\n",
    "From the plot above and result of the permutation test, the 0.05% significance level of the simulated statistics is 0.15 and our observed test statistics 0.1, which is smaller that the values of the significance level. The p-value is 0.411, which is greater than the significance level of 0.05. So we **fail to reject the null hypothesis.**\n",
    "\n",
    "Thus, we conclude that **the missingness of `descrpition` likely depends on the `minutes` column.**\n",
    "\n",
    "---\n",
    "\n",
    "## Hypothesis Testing\n",
    "\n",
    "The question we are going to explore and reasearch in this section is the following: **Do popular recipes (those with high review count) have a lower sugar level compared to less popular ones (those with low review count)** \n",
    "\n",
    "Recall that we define high review count threshold as having more than **10** reviews. We will conduct a permutationt test to see if the distribution of sugar level for popular recipes and the distribution of sugar level for not popular recipes are similar. \n",
    "\n",
    "Set up:\n",
    "\n",
    "**Null Hypothesis**: Receipes with a high review count do not have a lower sugar levels than those with low review count. Any observed differences in our samples are merely due to random chance.\n",
    "\n",
    "**Alternative Hypothesis**: Recipes with high review count indeed have a lower sugar level than those with a low review count. The observed difference observed in our samples cannot be explained by random chance alone.\n",
    "\n",
    "**Test Statistics**: Since our variable of interest is numerical and our test is one-tail test, signifying a directional alternative hypothesis, we will use difference in mean as our test statistics for the permutation test.\n",
    "\n",
    "**Significance Level**: 0.05\n",
    "\n",
    "<br>\n",
    "\n",
    "We created a new column called `is_popular` which is true if the recipe has reivew count more than 10, and false otherwise. \n",
    "\n",
    "This is a sample of the dataframe that we are going to perform permutation test on. \n",
    "\n",
    "|      |   sugar | is_popular   |\n",
    "|------|--------|-------------|\n",
    "|  1092 |      36 | False        |\n",
    "| 44285 |       4 | False        |\n",
    "| 60708 |      18 | False        |\n",
    "| 56138 |      48 | False        |\n",
    "|   628 |       0 | True         |\n",
    "\n",
    "<br>\n",
    "\n",
    "We shuffle the `is_popular` 1000 times and get 1000 simulated difference in mean test statistics of `sugar` column during the permutation test. The emperical distribution of permutation test result is shown as below. \n",
    "\n",
    "<iframe src=\"figures/hyp.html\" width=800 height=600 frameBorder=0></iframe>\n",
    "\n",
    "**P-value**: 0.0\n",
    "\n",
    "From looking at the graph above, we can see that our observed difference in mean, which is 14.58, is greater than the significance level of simulated difference in mean, which is 6.05, suggesting that the observed statistics in our sample are not merely coincidental. Furthermore, the p-value obtained from our permutation testing is 0.0, which falls below our significance level of 0.05. Therefore, we **reject our null hypothesis** and in favor of our alternative hypothesis: recipes with high review counts likely have a lower sugar level compared to those with low review counts.\n",
    "\n",
    "The result can be reasonable that people might like to give high rating to recipes with lower sugar content in food, and food with a lower sugar level is considered more healthy. \n",
    "\n",
    "---\n",
    "\n",
    "## Framing a Prediction Problem\n",
    "\n",
    "Recall from the introduction that we are interesting in the following problem:\n",
    "\n",
    "**What category does a recipe fall into based on its predicted popularity level and average rating score?**\n",
    "\n",
    "Category 1: Low review count, low average rating\n",
    "\n",
    "Category 2: Low review count, high average rating\n",
    "\n",
    "Category 3: High review count, low average rating\n",
    "\n",
    "Category 4: High review count, high average rating\n",
    "\n",
    "Note: we define the threshold for high and low review counts as **10** reviews count and the threshold for high and low average rating scores as **4.8**\n",
    "\n",
    "Specifically, we want to classify recipes into the above 4 categories based on all other information we have. \n",
    "\n",
    "<br>\n",
    "\n",
    "The prediction problem we are addressing is a **multi-class classification** problem since our goal is to classify recipes into one of four distinct categories. These class categories are determined by two other variables `rating` which categorizes recipes based on their average rating and `n_review` which categorizes recipes based on the number of reviews it has. Thus, recipes are classified as a combination of either having a low review count or a high review count, and either a low average rating or a high average rating, resulting in four unique class labels.\n",
    "\n",
    "<br>\n",
    "\n",
    "To generate features for our classification model, the variables we will be using are all other columns except `rating` and `n_review` since these two variables are used to create the class categories. \n",
    "\n",
    "The key metrics we will be using for evaluating our classifier model performance are **accuracy, precision, recall, and f1-score**:\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"figures/accuracy.png\" alt=\"accuracy formula\">\n",
    "\n",
    "- **Accuracy** represents the proportion of correctly classified instances among all observations. It tells us the overall performance of how the model classifies recipes into each category. However, accuracy does not tell the full story, especially when dealing with imbalanced data.\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"figures/precision.png\" alt=\"precision formula\">\n",
    "\n",
    "- **Precision** measures the proportion of the predicted positive instances that are correctly classified. It tells us how good the model is at avoiding false positive predictions. In our content, a false positive occurs when low-rating recipes are mistakenly classified into high-rating categories. A high precision minimizes the occurrence of such misclassification. \n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"figures/recall.png\" alt=\"recall formula\">\n",
    "\n",
    "- **Recall** measures the proportion of the actual positive instances that are correctly classified. It tells us how good the model is at identifying all the positive instances that are present, without missing too many of them. In our context, it reflects the model’s ability to correctly classify all high-rating recipes into the high-rating categories. A high recall maximizes the occurrence of the correct classification. \n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"figures/f1-score.png\" alt=\"f1-score formula\">\n",
    "\n",
    "- **F1-score**, being the harmonic mean of precision and recall, provides a balanced summary of the model’s predictive power, considering both predicted positive and actual positive. \n",
    "\n",
    "\n",
    "## Baseline Model: A Simple Approach\n",
    "\n",
    "We split our data to training and testing set by stratifing using the class label. The training set consititue 80% of our data while the testing has the reamining 20% of our data. We will use testing set to evalute the ability of our model to generalize to unseen data.:\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(recipe.drop(['class', 'rating', 'n_review'], axis=1), \n",
    "                                                        recipe['class'], test_size=0.2, stratify=recipe['class'])\n",
    "\n",
    "### Feature Engineering\n",
    "\n",
    "We perform the following feature engineering steps to transform our vairables before fitting into our model. We use RobustScaler instead of StandardScaler on most numerical column because there exist extreme values in those columns. RobustScaler use the median instead of the mean while scaling the data. \n",
    "\n",
    "<br>\n",
    "\n",
    "**Quantative Feature**:\n",
    "\n",
    "`minutes`, `protein`, `sodium`, `saturated_fat`, `total_fat`, `carbohydrates`\n",
    "- Type: quantative continuous\n",
    "- Feature Transformation: use RobustScaler to reduce the impact of outlier\n",
    "\n",
    "`n_steps`, `n_ingredients`\n",
    "- Type: quantative discrete\n",
    "- Feature Transformation: passthrough\n",
    "\n",
    "`time`\n",
    "- Type: quantative\n",
    "- Feature Transformation: extract year, month, and day from `submitted` timestamp column\n",
    "\n",
    "<br>\n",
    "\n",
    "**Categorical Feature**:\n",
    "\n",
    "`calories`, `sugar`\n",
    "- Type: quantative to nominal\n",
    "- Feature Transformation: cateogrize calories and sugar into 8 bins and do one-hot encoding\n",
    "\n",
    "`recipe_complexity`\n",
    "- Type: nominal\n",
    "- Feature Transformation: binarize `n_steps` and `n_ingredients` using the threshold of 10 to represent recipe complexity\n",
    "\n",
    "We choose to build features from the above columns as we believe these features might have some relationship for predicting recipe's popularity and average rating. \n",
    "\n",
    "---\n",
    "\n",
    "### Baseline Model Building and Performance Evaluation\n",
    "\n",
    "For Baseline Model, we decide to use the Random Forest model for our classification problem. \n",
    "\n",
    "The main idea of Random Forest algorithm is to Fit n number of decision trees by using bagging and a random subset of features at each split. Predict by taking a vote from those n decision trees. It is the idea of Ensemble Learning.\n",
    "\n",
    "<br>\n",
    "\n",
    "Here is a pipeline of our baseline model, in which we transform our column first, then fit into RandomForestClassifier. \n",
    "\n",
    "    Pipeline(steps=[('col_trans',\n",
    "                     ColumnTransformer(transformers=[('outlier', RobustScaler(),\n",
    "                                                      ['minutes', 'protein',\n",
    "                                                       'sodium', 'saturated_fat',\n",
    "                                                       'total_fat',\n",
    "                                                       'carbohydrates']),\n",
    "                                                     ('pass', 'passthrough',\n",
    "                                                      ['n_steps', 'n_ingredients']),\n",
    "                                                     ('to_bin',\n",
    "                                                      Pipeline(steps=[('outlier',\n",
    "                                                                       RobustScaler()),\n",
    "                                                                      ('to_bins',\n",
    "                                                                       KBinsDiscretizer(n_bins=8))]),\n",
    "                                                      ['calories', 'sugar']),\n",
    "                                                     ('time',\n",
    "                                                      Pipeline(steps=[('time',\n",
    "                                                                       FunctionTransformer(func=<function <lambda> at 0x2b514b550>))]),\n",
    "                                                      ['submitted']),\n",
    "                                                     ('complexity',\n",
    "                                                      Pipeline(steps=[('complex',\n",
    "                                                                       FunctionTransformer(func=<function recipe_complexity at 0x2b5056ee0>))]),\n",
    "                                                      ['n_steps',\n",
    "                                                       'n_ingredients'])])),\n",
    "                    ('clf', RandomForestClassifier())])\n",
    "\n",
    "<br>\n",
    "\n",
    "After fitting our training data into the baseline model, we evalute our model using testing data. The confusion matrix below is the result of the prediction of testing data. \n",
    "\n",
    "<iframe src=\"figures/base_confus.html\" width=800 height=600 frameBorder=0></iframe>\n",
    "\n",
    "\n",
    "By looking at the confusion matrix, we can see that the baseline model misclassifies lots of recipe into class lable 1 and 2. In addition, it have difficult in accurately classifying recipes into class label 3 and 4, which correspond to high review counts. This is caused by the imbalanced nature of class label in our dataset. **Recipes with high review counts are relatively rare and constitute a minority group of data**, whereas recipes with low review count are more prevalent and make up a majority of the data. This is something we will address in building our final model. \n",
    "\n",
    "Result of counting all the class lable in our dataset, showing imbalanced data:\n",
    "\n",
    "    2    48979\n",
    "    1    28825\n",
    "    4     1834\n",
    "    3     1535\n",
    "    Name: class, dtype: int64\n",
    "\n",
    "<br>\n",
    "\n",
    "Let's look at the precision, recall, and f1-score of our baseline model for predicting unseen data. \n",
    "\n",
    "\n",
    "|            | precision | recall | f1-score | support |\n",
    "|------------|-----------|--------|----------|---------|\n",
    "|   1        |   0.42    |  0.13  |   0.20   |  5765   |\n",
    "|   2        |   0.61    |  0.91  |   0.73   |  9796   |\n",
    "|   3        |   0.00    |  0.00  |   0.00   |   307   |\n",
    "|   4        |   0.00    |  0.00  |   0.00   |   367   |\n",
    "|accuracy    |           |        |   0.59   | 16235   |\n",
    "|macro avg   |   0.26    |  0.26  |   0.23   | 16235   |\n",
    "|weighted avg|   0.52    |  0.59  |   0.51   | 16235   |\n",
    "\n",
    "We see that the model has 0 precision, recall, and f1-score for class label 3 and 4 while it has a high f1-score for class label 2 since most recipes belongs to class 2. Our model accuracy is 59%. Overall, the performane of our baseline model is not as good as we thought so since it has difficult in identifying popular recipes, which are something we are more interested in. We will improve our baseline model to correctly classify more popular recipes. \n",
    "\n",
    "---\n",
    "\n",
    "## Final Model: Balanced Random Forest & Binary Classification\n",
    "\n",
    "As usual, we will us the same training and testing data from baseline mode. \n",
    "\n",
    "#### Feature Engineering\n",
    "\n",
    "**The following features are from the baseline model**\n",
    "\n",
    "`minutes`, `protein`, `sodium`, `saturated_fat`, `total_fat`, `carbohydrates`\n",
    "- Type: quantative continuous\n",
    "- Feature Transformation: use RobustScaler to reduce the impact of outlier\n",
    "\n",
    "`n_steps`, `n_ingredients`\n",
    "- Type: quantative discrete\n",
    "- Feature Transformation: passthrough\n",
    "\n",
    "`time`\n",
    "- Type: quantative\n",
    "- Feature Transformation: extract year, month, and day from `submitted` timestamp column\n",
    "\n",
    "`recipe_complexity`\n",
    "- Type: nominal\n",
    "- Feature Transformation: binarize `n_steps` and `n_ingredients` to represent recipe complexity\n",
    "\n",
    "<br>\n",
    "\n",
    "**The following text features are added for the final model:**\n",
    "\n",
    "`description`\n",
    "- Type: text data\n",
    "- Feature Transformation: \n",
    " - Build a list of vocabulary from high Inverse Document Frequency(IDF) words from the `description` of **high reivew count recipes** (minority class label, class 3 and 4)\n",
    " - Vectorize the description text column using TF-IDF and the vocabulary from previous step\n",
    " - For each recipe, extract the top 5 highest TF-IDF values as the features\n",
    "\n",
    "`steps`\n",
    "- Type: text data\n",
    "- Feature Transformation: \n",
    " - Build a list of vocabulary from high Inverse Document Frequency(IDF) words from the `steps` of **high reivew count recipes** (minority class label, class 3 and 4)\n",
    " - Vectorize the steps text column using TF-IDF and the vocabulary from previous step\n",
    " - For each recipe, extract the top 5 highest TF-IDF values as the features \n",
    "\n",
    "<br>\n",
    "\n",
    "We believe that incorporating `description` and `steps` feature can improve our model's ability to identify recipes with high review counts since we built a vocabulary from those recipes and use TF-IDF to extract important text information. While there is a potential risk of data leakage since we use the class label to build feature, our approach to using the class label is indirect and we only use the training data to build vocabulary. We believe it is appropraite to use these text feature in our final model. \n",
    "\n",
    "<br>\n",
    "\n",
    "Simple sentiment analysis on `reviews` column:\n",
    "\n",
    "`reviews`\n",
    "- Type: text data\n",
    "- Feature Transformation:\n",
    " - Manaually create a list of sentiment words such as `good`, `loved`, `hated`, etc that are relevant for extracting sentiment information\n",
    " - For each word in sentiment word list, binarize `reviews` based on whether there are enough review comments that contain that particular word\n",
    " \n",
    "We think that adding this sentiment analysis on `reviews` feature can as well improve our model's ability to distinguish recipes between high average rating and low average rating by the simple logic that high rating recipes often have more positive words while low rating recipe often have more negative words. \n",
    "\n",
    "---\n",
    "\n",
    "### Final Model Building\n",
    "\n",
    "The approach we aim to use for our final model is to decompose our multi-class classification problem into two distinct binary classification problems. We will construct two separate classification models, one to determine whether a recipe has a high or low review count, and another to determine whether a recipe has a high or low average rating. Subsequently,  we will combine the outcomes of these two binary classifiers to generate the four different class labels corresponding to the result of our multi-class classification problem.\n",
    "\n",
    "We observed previously that there is a severe data imbalance issue in classifying recipes based on review count, where the low review count category represents the majority class, while the high review count category represents the minority class. To address this imbalance, the model we are going to use is the **BalancedRandomForest** algorithm from the imbalanced-learn library. Unlike the standard random forest implemented in sklearn, balanced random forest us bootstrapping to sample from the minority class and randomly selects the same number of samples with replacement from the majority class while constructing each decision tree. This approach will help our model’s ability to have a higher precision in classifying the minority class. \n",
    "\n",
    "A simple model construction of our idea:\n",
    "\n",
    "    class ClassificationTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "        def __init__(self, model_1, model_2):\n",
    "            self.model_1 = clone(model_1)\n",
    "            self.model_2 = clone(model_2)\n",
    "\n",
    "        def fit(self, X, Y):\n",
    "            self.model_2.fit(X, Y[1])\n",
    "            self.model_1.fit(X, Y[0])\n",
    "\n",
    "            return self\n",
    "\n",
    "        def predict(self, X):\n",
    "            y_1 = self.model_1.predict(X)\n",
    "            y_2 = self.model_2.predict(X)\n",
    "            \n",
    "            # combine the outcome of y_1 and y_2 \n",
    "            ...\n",
    "            \n",
    "            return y\n",
    "\n",
    "<br>\n",
    "\n",
    "Model pipeline: \n",
    "\n",
    "    Pipeline(steps=[('col_trans',\n",
    "                     ColumnTransformer(transformers=[('outlier', RobustScaler(),\n",
    "                                                      ['minutes', 'protein',\n",
    "                                                       'sodium', 'saturated_fat',\n",
    "                                                       'total_fat',\n",
    "                                                       'carbohydrates']),\n",
    "                                                     ('pass', 'passthrough',\n",
    "                                                      ['n_steps', 'n_ingredients']),\n",
    "                                                     ('time',\n",
    "                                                      Pipeline(steps=[('time',\n",
    "                                                                       FunctionTransformer(func=<function <lambda> at 0x2b514b550>))]),\n",
    "                                                      ['submitted']), ...])),\n",
    "                    ('clf',\n",
    "                     ClassificationTransformer(model_1=BalancedRandomForestClassifier(class_weight='balanced',\n",
    "                                                                                      criterion='entropy',\n",
    "                                                                                      max_depth=16,\n",
    "                                                                                      n_estimators=180),\n",
    "                                               model_2=RandomForestClassifier(class_weight='balanced',\n",
    "                                                                              criterion='entropy',\n",
    "                                                                              max_depth=19,\n",
    "                                                                              n_estimators=150)))])\n",
    "\n",
    "We use standard random forest to classify recipes based on average rating and use balanced random forest to classify recipes based on review counts, since it has more imbalanced data. \n",
    "\n",
    "---\n",
    "\n",
    "### Hyperparameter Tunning\n",
    "\n",
    "We manually iterative through a list of hyperparameter with stratified 5-fold trian-test split seperately for our two classifier models to find the best hyperparameter for model accuracy. We found the best `max_depth` hyperparameter to be 16 and 19 and the best `num_estimators` hyperparameter to be 180 and 150 for balanced ranfom forest and standard random forest, respectively, as seen in our pipeline above. \n",
    "\n",
    "---\n",
    "\n",
    "### Model Performance Evaluation\n",
    "\n",
    "After fitting our training data into the final model, we evalute our model using testing data. The confusion matrix below is the result of the prediction of testing data. \n",
    "\n",
    "<iframe src=\"figures/final_confus.html\" width=800 height=600 frameBorder=0></iframe>\n",
    "\n",
    "As seen in the confusion matrix, our final model has correctly classified a considerable amount of minority class label, class 3 and 4, which is overall an improvement to the baseline model. \n",
    "\n",
    "Let's look at the precision, recall, and f1-score of our final model for predicting unseen data.\n",
    "\n",
    "|            | precision | recall | f1-score | support |\n",
    "|------------|-----------|--------|----------|---------|\n",
    "|   1        |   0.59    |  0.47  |   0.53   |  5765   |\n",
    "|   2        |   0.74    |  0.73  |   0.73   |  9796   |\n",
    "|   3        |   0.19    |  0.55  |   0.29   |   307   |\n",
    "|   4        |   0.19    |  0.59  |   0.29   |   367   |\n",
    "|accuracy    |           |        |   0.63   | 16235   |\n",
    "|macro avg   |   0.43    |  0.58  |   0.46   | 16235   |\n",
    "|weighted avg|   0.66    |  0.63  |   0.64   | 16235   |\n",
    "\n",
    "\n",
    "Although the precision for class 3 and 4 are relatively low, their recall are high, which means that **our model is good at capturing all the high reiview count recipes, without missing too many of them, but at the same time, it makes too many false positive, misclassify low review count recipes into high review count categories.** The trade off is acceptaible. The accuracy of our final model is 63%, which is better than our baseline model. Overall, our final model is an improvement over the baseline model. \n",
    "\n",
    "<br>\n",
    "\n",
    "<iframe src=\"figures/final_feature.html\" width=800 height=600 frameBorder=0></iframe>\n",
    "\n",
    "We can gain insights into our model's feature importance by visualization. Feature importance highlights the extent to which our engineered feature contributes to help the model's classification decision. A higher importance score indicates that the feature plays a more significant role in classifying recipes. We can see from the plot that certain features have a very high importance to the model. Specifically, the `review` feature corresponds to features numbered from 21-50. Recall that we use a list of manually created sentiment words for simple sentiment analysis. The result of the visualization above indicates that certain sentiment words are particularly useful in helping the model to make accurate predictions. \n",
    "\n",
    "Below, we present the top 10 most useful sentiment words for feature engineering in our classification model:\n",
    "\n",
    "    1        great\n",
    "    2         good\n",
    "    3         very\n",
    "    4    delicious\n",
    "    5         made\n",
    "    6          but\n",
    "    7        loved\n",
    "    8      perfect\n",
    "    9        would\n",
    "    10    wonderful\n",
    "\n",
    "<br>\n",
    "\n",
    "Finally, we fit our final model using all availbale data and ship to production for fairness analysis. \n",
    "\n",
    "    final_model = pl_clf.fit(\n",
    "        recipe.drop(['class', 'rating', 'n_review'], axis=1),\n",
    "        [to_n_review(recipe['class']), to_rating(recipe['class'])])\n",
    "\n",
    "--- \n",
    "\n",
    "## Fairness Analysis\n",
    "\n",
    "For fairness analysis, we are interested in this question: **“Are recipes with `vegetarian` tags more likely to be correctly classified as to the high average rating category by the model, compared to those without the `vegetarian` tags?”** Are our model fair in terms of precision?\n",
    "\n",
    "To evaluate fairness, we will compare the **precision** acorss two distinct groups. Specifically, we will compare the precision score for recipes with the 'vegetarian' tag against those without it. If the precision for recipes with the `vegetarian` tag is statistically significantly higher than the precision for recipes without it, it could potentially indicate a bias towards classifying `vegetarian` recipes as high average rating more frequently, even when they should not be classified as such. In our dataset, a high average rating corresponds to class categories 2 and 4. \n",
    "\n",
    "<br>\n",
    "\n",
    "Setup:\n",
    "\n",
    "**Null Hypothesis**: Our model is fair. Our classifier's precision is the same for recipes with and without `vegetarian` tag, and any differences are due to random chance.\n",
    "\n",
    "**Alternative Hypothesis**: Our model is not fair. Our classifier's precision is higher for recipes with `vegetarian` tag than those without, and any observed differences can not be explained by random chance alone.\n",
    "\n",
    "**Test statistic**: Difference in average precision of class 2 and 4 (without `vegetarian` tag - with `vegetarian` tag).\n",
    "\n",
    "**Significance level**: 0.05.\n",
    "\n",
    "<br>\n",
    "\n",
    "We fitted our final model with all available and created a new column `has_tags_vegetarian` that indicates if the recipes has `vegetarian` tag. \n",
    "\n",
    "We then shuffle the `has_tags_vegetarian` 1000 times and get 1000 simulated difference in average precision test statistics for recipes with and without `vegetarian` tag during the permutation test. The emperical distribution of permutation test result is shown as below. \n",
    "\n",
    "<iframe src=\"figures/fair.html\" width=800 height=600 frameBorder=0></iframe>\n",
    "\n",
    "From the graph above, we can see that the observed difference in precision falls below the significance level of 0.05, suggesting that the observed statistics in our sample are likely by random chance alone. The p-value we obtained from performing our permutation testing is approximately 0.397, which is greater than our significance level of 0.05. Therefore, we **fail to feject our null hypothesis** that the precision of our classifier is likely around the same for recipes with and without `vegetarian` tag, and any observed differences are due to random chance. Our model **achieves precision parity** across groups with and without `vegetarian`tag. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c32e538",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Nowadays, cooking and sharing recipes online has become widely popular and a significant part of many people’s lives. With the convenience of online recipe platforms like food.com, millions of users benefit from having access to a vast collection of recipes in a variety of culinary. Among all the recipe options available, however, some recipes stand out as both particularly popular and highly rated. \n",
    "\n",
    "In this project, we dive into an extensive collection of food recipe data sourced from food.com. The first dataset we are going to use contains food recipes from 2008 to 2018 on food.com, with detailed information about a recipe such as ingredients, preparation times, nutritional values, etc. This dataset contains 83782 observations, meaning that there are 83782 unique recipes in total in our data. The second dataset contains information about users’ review comments and ratings submitted for the recipes in the first dataset. This dataset contains 731927 number of reviews in total submitted by users. Some recipes have lots of reviews while others might have fewer. \n",
    "\n",
    "<br>\n",
    "\n",
    "Let’s take a closer look at the information we have. Below is the description of some of the relevant columns in the first dataset:\n",
    "\n",
    "\n",
    "|      Column      |       Description         |\n",
    "|------------------|       ------------       |\n",
    "|   `name`         |       Recipe name         |\n",
    "|   `minutes`      |       Minutes to prepare food |\n",
    "|   `submitted`    |       Date recipe was uploaded |\n",
    "|   `tags`         |       Food.com tags for recipe |\n",
    "|   `nutrition`    |      Nutrition information for the recipe, <br> including calories, total fat, sugar, sodium <br>protein, saturated fat, and carbohydrates |\n",
    "|   `steps` |  Steps to make the food by the recipe steps    |\n",
    "|   `description` |   Recipe description    |\n",
    "|   `ingredients` |      List of ingredients for recipe  |\n",
    "\n",
    "\n",
    "Let’s take a look at the second dataset:\n",
    "\n",
    "|      Column      |       Description         |\n",
    "|-----------------|       ------------          |\n",
    "|   `date`         |       Date reviews was submitted       |\n",
    "|   `rating`       |       Rating given by the user         |\n",
    "|   `review`       |       Review comment given by the user |\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "By combining these two comprehensive datasets, we aim to answer the following research questions throughout the project: **What are some of the characteristics of the recipe that are both popular and highly rated? What category does a recipe fall into based on its predicted popularity level and average rating score?**\n",
    "\n",
    "Category 1: Low review count, low average rating\n",
    "\n",
    "Category 2: Low review count, high average rating\n",
    "\n",
    "Category 3: High review count, low average rating\n",
    "\n",
    "Category 4: High review count, high average rating\n",
    "\n",
    "Note: we define the threshold for high and low review counts as **10** reviews count and the threshold for high and low average rating scores as **4.8**\n",
    "\n",
    "<br>\n",
    "\n",
    "Through a rigorous analysis of these datasets, we will explore univariate and bivariate relationships between key variables, investigate missing values in the dataset, conduct hypothesis testing to answer the question of interest, build and evaluate a classifier model, and ultimately conduct fairness analysis for the final model. This project will provide valuable insights into understanding the key characteristics of certain recipes that stand out as exceptional and loved by many. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83739f4",
   "metadata": {},
   "source": [
    "## Data Clearning and EDA\n",
    "\n",
    "### Data Cleaning \n",
    "\n",
    "We will perform the following data cleaning steps to combine and transform our dataset into tidy format, before diving into analysis. \n",
    "\n",
    "1. **Convert data type**: We converted the `steps`, `tags`, and `ingredients` columns from the string data type to the list data type. This will help us in accessing each individual element. We also converted the `submitted` column to datatime format. \n",
    "\n",
    "2. **Expand the `Nutrition` column**: We expanded the `nutrition` column in the Dataframe such that nutritional information like `calories`, `total fat`, `sugar`, etc is in their individual columns. This allows us to easily access and analyze each quantitative column separately. \n",
    "\n",
    "3. **Merge the two datasets into one**: We merged the review dataset into the recipes dataset such that each unique recipe has an average rating and a review list where each element represents a review comment given by a user. We added a new column called `n_review` that counts the number of reviews a recipe has. We will use this information to represent the popularity of the recipe. \n",
    "\n",
    "<br>\n",
    "\n",
    "After data cleaning, we have the following Dataframe comes in handy for analysis: \n",
    "\n",
    "| name |   id | minutes | contributor_id | submitted  | tags  | n_steps | steps  | description  | ingredients | n_ingredients | calories | total_fat | sugar | sodium | protein | saturated_fat | carbohydrates | rating | n_review | reviews   |\n",
    "|-----------------------------|---------|---------|----------------|------------|------------------------------------------------------------------------------------------------------------------------------------------------------|---------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------|--------------|----------|-----------|-------|--------|---------|---------------|---------------|--------|----------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| 1 brownies in the world    best ever |  333281 |      40 |         985201 | 2008-10-27 | ['60-minutes-or-less', 'time-to-make', 'course', 'main-ingredient']                                                                                 |      10 | ['heat the oven to 350f and arrange the rack in the middle ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | these are the most; chocolatey, moist, rich, dense, fudgy, delicious brownies ...                                                                                                                                                       | ['bittersweet chocolate', 'unsalted butter', 'eggs', 'granulated sugar']                                                                       |            9 |    138.4 |      10.0 |  50.0 |    3.0 |     3.0 |           19.0 |             6.0 |    4.0 |      1.0 | ['These were pretty good, but took forever to bake ...                                                                                                                                                                                                                                                                            |\n",
    "| 1 in canada chocolate chip cookies   |  453467 |      45 |        1848091 | 2011-04-11 | ['60-minutes-or-less', 'time-to-make', 'cuisine', 'preparation', 'north-american' ... |      12 | ['pre-heat oven the 350 degrees f, in a mixing bowl ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | this is the recipe that we use at my school cafeteria for chocolate chip cookies ...                                                                                                                                                       | ['white sugar', 'brown sugar', 'salt', 'margarine', 'eggs']                                                                                      |           11 |    595.1 |      46.0 | 211.0 |   22.0 |    13.0 |           51.0 |            26.0 |    5.0 |      1.0 | ['Originally I was gonna cut the recipe in half (just the 2 of us here) ...                                                                                                                                                                                                                                                     |\n",
    "| 412 broccoli casserole        |  306168 |      40 |          50969 | 2008-05-30 | ['60-minutes-or-less', 'time-to-make', 'course', 'main-ingredient']                                                                                 |       6 | ['preheat oven to 350 degrees, spray a 2 quart baking dish with cooking spray ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | since there are already 411 recipes for broccoli casserole ...                                                                                                                                                         | ['frozen broccoli cuts', 'cream of chicken soup', 'sharp cheddar cheese']                                                                       |            9 |    194.8 |      20.0 |   6.0 |   32.0 |    22.0 |           36.0 |             3.0 |    5.0 |      4.0 | ['This was one of the best broccoli casseroles that I have ever made ...                                                                                                                                                                                                                                                         |\n",
    "| millionaire pound cake       |  286009 |     120 |         461724 | 2008-02-12 | ['time-to-make', 'course', 'cuisine', 'preparation', 'occasion', 'north-american']                                                                   |       7 | ['freheat the oven to 300 degrees, grease a 10-inch tube pan with butter ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | why a millionaire pound cake?  because it's super rich ...                                                                                                                                                                              | ['butter', 'sugar', 'eggs', 'all-purpose flour', 'whole milk']                                                                                 |            7 |    878.3 |      63.0 | 326.0 |   13.0 |    20.0 |          123.0 |            39.0 |    5.0 |      1.0 | ['don't let the calories and fat grams scare you off ...                                                                                                                                                                                                                                                                           |\n",
    "| 2000 meatloaf                |  475785 |      90 |        2202916 | 2012-03-06 | ['time-to-make', 'course', 'main-ingredient', 'preparation', 'main-dish' ... |      17 | ['pan fry bacon , and set aside on a paper towel to absorb excess grease ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | ready, set, cook! special edition contest entry: ...                                                                                                                                                                                     | ['meatloaf mixture', 'unsmoked bacon', 'goat cheese', 'unsalted butter']                                                                        |           13 |    267.0 |      30.0 |  12.0 |   12.0 |    29.0 |           48.0 |             2.0 |    5.0 |      2.0 | ['Delicious!!!!! -- the goat cheese made the difference ...                                                                                                                                                                                                                                                                       |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0332db0f",
   "metadata": {},
   "source": [
    "### Univariate Analysis\n",
    "\n",
    "In the univariate analysis, we will look at the distribution of the calories and the distribution of the rating in food recipes in the dataset. \n",
    "\n",
    "<br>\n",
    "\n",
    "#### Distribution of Calories\n",
    "\n",
    "<iframe src=\"figures/uni_var_1.html\" width=800 height=600 frameBorder=0></iframe>\n",
    "\n",
    "(This is a histogram of the distribution of the calories of food recipes in the dataset.)\n",
    "\n",
    "We only included 98% of the data values from the `calories` column for better visualization. From looking at the histogram, we see that most of the calorie values range from 0 to 1000, with fewer data values towards the right side of the plot. The distribution shows right-skewness, indicating that a significant proportion of food recipes fall within a typical calorie range, with some recipes featuring higher calorie counts.\n",
    "\n",
    "---\n",
    "\n",
    "#### Distribution of Rating\n",
    "\n",
    "<iframe src=\"figures/uni_var_2.html\" width=800 height=600 frameBorder=0></iframe>\n",
    "\n",
    "(This is a histogram of the distribution of the rating of food recipes in the dataset.)\n",
    "\n",
    "As we look at the histogram, the majority of recipes have an average rating above 3.0, with only a very small minority falling below this threshold. If we want to develop a classification model to distinguish recipes based on good and bad ratings, we must address the issue of **imbalanced data**. This is something we should consider when building our final classification model. \n",
    "\n",
    "---\n",
    "\n",
    "### Bivariate Analysis\n",
    "\n",
    "For bivariate analysis, we will look at the relationship between sugar percent daily value and rating range, as well as, calories percent daily value and the number of ingredients in food recipes in the dataset. \n",
    "\n",
    "<br>\n",
    "\n",
    "#### Sugar vs. Rating\n",
    "\n",
    "<iframe src=\"figures/bi_var_1.html\" width=800 height=600 frameBorder=0></iframe>\n",
    "\n",
    "(This is a box plot featuring the relationship between rating and sugar percent daily value in food recipes in the dataset.)\n",
    "\n",
    "In crafting this box plot, we only incorporated 95% of the data values from the `sugar` column to exclude extreme values and grouped ratings into bins for better visualization. As we look into the box plot, we note a slight upward trend in the sugar daily level for the recipes as the average rating decreases, particularly in the third quarter and the upper fence. This observation suggests a potential **negative correlation between the sugar content and the average rating in recipes**, indicating that low sugar content might tend to receive higher ratings, and vice versa. \n",
    "\n",
    "---\n",
    "\n",
    "#### Calories vs. Number of Ingredients\n",
    "\n",
    "<iframe src=\"figures/bi_var_2.html\" width=800 height=600 frameBorder=0></iframe>\n",
    "\n",
    "(This is a scatter plot featuring the relationship between calorie percent daily value and the number of ingredients in food recipes in the dataset.)\n",
    "\n",
    "\n",
    "In making this visualization, we only included 99% of the data value for the `calories` column to exclude extreme values, grouped ratings into bins, and introduced random noise to the `n_ingredients` column, which represents a discrete variable, for better visualization. From examining the scatter plot, we see that there exists a **week positive correlation between the calorie daily level and the number of ingredients in recipes**. This finding aligns with our expectation that using a variety of ingredients in a recipe would likely result in higher calorie content in food.\n",
    "\n",
    "---\n",
    "\n",
    "### Data Aggregation\n",
    "\n",
    "So far, we only looked at the overall distribution of one variable and the bivariate distribution of two variables. For a better understanding of our data, we construct the following pivot table that shows the aggregated distribution of quantitative variables by mean, conditional on the number of ingredients. As we examine each column of the pivot table from top to bottom, we can observe a consistently increasing trend in various quantitative variables such as `calories`, `carbohydrates`, `minutes`, `n_steps`, `protein`, `saturated_fat`, `sodium`, and `total_fat`, as the number of ingredients increases in food recipes. This suggests that `n_ingredients` might have a positive correlation with these quantitative variables. \n",
    "\n",
    "\n",
    "The pivot table is shown below (number is rounded to 3 decimal places)\n",
    "\n",
    "|   n_ingredients |   calories |   carbohydrates |   minutes |   n_steps |   protein |   saturated_fat |   sodium |   sugar |   total_fat |\n",
    "|----------------|-----------|----------------|----------|----------|----------|----------------|---------|--------|------------|\n",
    "|               1 |    288.77  |          10.2   |    47.6   |     7.3   |    10.8   |          25     |   12.9   | 100.1   |      21.8   |\n",
    "|               2 |    238.312 |           8.728 |    56.131 |     5.711 |    13.18  |          18.211 |    7.699 |  62.322 |      15.27  |\n",
    "|               3 |    233.924 |           8.413 |    42.754 |     5.485 |    13.873 |          19.791 |   10.844 |  55.622 |      15.406 |\n",
    "|               4 |    263.534 |           9.084 |    40.107 |     6.135 |    16.943 |          24.627 |   12.858 |  60.599 |      18.164 |\n",
    "|               5 |    282.587 |           9.288 |    49.087 |     7.104 |    19.914 |          26.891 |   15.712 |  54.497 |      20.726 |\n",
    "|               6 |    300.474 |           9.7   |    51.857 |     7.716 |    22.256 |          28.382 |   17.644 |  52.118 |      22.189 |\n",
    "|               7 |    326.27  |           9.87  |    56.538 |     8.413 |    26.021 |          31.473 |   19.369 |  46.82  |      25.221 |\n",
    "|               8 |    339.984 |          10.234 |    58.947 |     9.214 |    28.029 |          32.541 |   20.882 |  45.587 |      26.283 |\n",
    "|               9 |    353.065 |          10.757 |    63.083 |     9.853 |    30.069 |          32.736 |   21.893 |  44.457 |      26.781 |\n",
    "|              10 |    366.854 |          10.979 |    67.319 |    10.709 |    32.143 |          34.049 |   22.93  |  45.52  |      27.997 |\n",
    "|              11 |    386.087 |          11.631 |    69.016 |    11.21  |    34.144 |          35.42  |   24.741 |  47.409 |      29.362 |\n",
    "|              12 |    402.617 |          12.329 |    72.786 |    11.912 |    35.873 |          36.136 |   25.677 |  49.844 |      30.143 |\n",
    "|              13 |    419.473 |          12.785 |    75.468 |    12.585 |    37.439 |          37.407 |   26.52  |  51.117 |      31.519 |\n",
    "|              14 |    437.62  |          13.139 |    81.278 |    13.326 |    39.786 |          39.264 |   28.078 |  51.233 |      33.134 |\n",
    "|              15 |    461.443 |          13.811 |    85.581 |    14.249 |    43.113 |          40.383 |   30.858 |  49.922 |      34.574 |\n",
    "|              16 |    486.545 |          14.736 |    82.214 |    14.924 |    45.352 |          42.702 |   33.729 |  51.809 |      36.217 |\n",
    "|              17 |    493.79  |          14.64  |    92.491 |    15.594 |    46.517 |          43.666 |   33.938 |  54.435 |      37.395 |\n",
    "|              18 |    532.985 |          15.415 |    98.909 |    16.39  |    50.407 |          47.228 |   36.299 |  54.46  |      40.908 |\n",
    "|              19 |    526.863 |          15.781 |    97.143 |    16.714 |    49.113 |          44.232 |   37.453 |  53.226 |      39.722 |\n",
    "|              20 |    549.916 |          15.232 |    99.352 |    17.654 |    52.645 |          47.735 |   40.738 |  47.756 |      43.358 |\n",
    "|              21 |    554.031 |          15.832 |   116.699 |    17.878 |    54.418 |          49.806 |   42.719 |  52.015 |      42.76  |\n",
    "|              22 |    586.774 |          17.954 |   131.708 |    19.685 |    56.208 |          49.023 |   41.831 |  64.338 |      43.069 |\n",
    "|              23 |    582.013 |          17.024 |   141.25  |    19.321 |    53.048 |          50.679 |   52.095 |  63.988 |      45.619 |\n",
    "|              24 |    628.248 |          17.029 |   130.754 |    19.884 |    63.217 |          46.072 |   44.623 |  49.623 |      49.681 |\n",
    "|              25 |    669.144 |          18.969 |   109.781 |    21.594 |    66.781 |          60.188 |   63.969 |  66.594 |      52     |\n",
    "|              26 |    531.532 |          17     |   104.12  |    18.36  |    47.28  |          44.72  |   42.96  |  52.12  |      39.48  |\n",
    "|              27 |    724.389 |          20.333 |   153.167 |    21.167 |    67.667 |          75.556 |   44.111 |  64.389 |      58     |\n",
    "|              28 |    537.787 |          16.133 |   111     |    28.333 |    61.467 |          38.467 |   52.733 |  54.533 |      37.2   |\n",
    "|              29 |    886.3   |          34     |    86.222 |    21.222 |    78     |          72.333 |   59.444 | 141.778 |      60     |\n",
    "|              30 |    631.656 |          17.333 |   116.667 |    20     |    55.222 |          62.222 |   51     |  41.222 |      53.222 |\n",
    "|              31 |    502.05  |          14.667 |   197.5   |    22.333 |    47.667 |          45     |   46.5   |  44.667 |      39.833 |\n",
    "|              32 |    697.35  |          18.5   |    55     |    34     |    66     |          87.5   |   53     |  30.5   |      58.5   |\n",
    "|              33 |    338.2   |          14     |    35     |     6     |     8     |          12     |   16     |  18     |      25     |\n",
    "\n",
    "<br>\n",
    "\n",
    "To get a visual representation of the above data, we have converted the pivot table to a series of line plots as below. \n",
    "\n",
    "<iframe src=\"figures/agg.html\" width=800 height=600 frameBorder=0></iframe>\n",
    "\n",
    "As we can see from the line plots above which illustrates the relationship between the number of ingredients and various other quantitative columns such as `calories`, `carbohydrates`, `minutes`, `n_steps`, `protein`, `saturated_fat`, `sodium`, and `total_fat`, we can observe clear and consistent increasing trends across all plots, aligns with our observation from the pivot table. These findings suggest a positive correlation between the number of ingredients and these quantitative variables, indicating that, on average, having more ingredients included in a food recipe corresponds to higher nutritional values, more preparation time, and a greater number of steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3edb2ab",
   "metadata": {},
   "source": [
    "## Assessment of Missingness\n",
    "\n",
    "\n",
    "### Not Missing At Random (NMAR) Analysis \n",
    "\n",
    "We believe that the missing values in the `rating` column are not Not Missing at Random (**NMAR**), meaning that the chance of a value being missing **depends on the actual missing values themselves.** Updon browersing from the food.com website, we found that there are certain reviews given by users without ratings in the comment section of a recipe (showing in the image below). This is the case in which users opt to provide a review comment, without giving a rating score to the recipe. Therefore, during the data generation process, this result in certain recipes having missing rating score values.\n",
    "\n",
    "<img src=\"figures/Image_1.png\" alt=\"NMAR example\">\n",
    "\n",
    "---\n",
    "\n",
    "### Missingness Dependency Analysis\n",
    "\n",
    "Besides the `rating` column, we also found missing values in `description` column. We hypothesize that the missingness of `description` is **Missing at Random (MAR)**, meaning that the chance of a value being missing **depends on some other columns**. We will investigate the missing dependency of `description` using permutation test. \n",
    "\n",
    "We hypothesize that the missingness of `description` column depends on the `sugar` column, meaning that there is some systemic difference between the distribution of sugar for those recipes missing the description and those that do not. \n",
    "\n",
    "<br>\n",
    "\n",
    "Set up:\n",
    "\n",
    "**Does the missingness of `description` depend on the `sugar` column?**\n",
    "\n",
    "**Null Hypothesis**: the missingness of `description` does not depend on `sugar`. \n",
    "\n",
    "**Alternative Hypothesis**: the missingness of `description` does depend on `sugar`.\n",
    "\n",
    "**Significance Level**: 0.05\n",
    "\n",
    "\n",
    "\n",
    "Let's look at the distribution of `sugar`, conditional on the missingness of `description` column. \n",
    "\n",
    "<iframe src=\"figures/miss_dist_1.html\" width=800 height=600 frameBorder=0></iframe>\n",
    "\n",
    "Based on the plot above, the orange line represents the distribution of sugar when `description` is not missing, while the blue line represents the distribution of sugar when `description` is missing. We can see that these two distributions of sugar, conditional on the missingness of `description`, looks quite difference. To quantify this difference, we decide to use Kolmogorov-Smirnov (K-S) statistic instead of the absolute difference of means. Therefore, we will use **K-S statistics** as our test statistics for the permutation test.\n",
    "\n",
    "\n",
    "\n",
    "We shuffle the missingness of description 1000 times and get 1000 simulated K-S test statistics of `sugar` column during the permutation test. The permutation result is shown as below. \n",
    "\n",
    "<iframe src=\"figures/miss_perm_1.html\" width=800 height=600 frameBorder=0></iframe>\n",
    "\n",
    "From the plot above and result of the permutation test, the 0.05% significance level of the simulated statistics is 0.16 and our observed test statistics 0.18, which is greater that the values of the significance level. The p-value is 0.019, which is less than the significance level of 0.05. So we **reject the null hypothesis.**\n",
    "\n",
    "Thus, we conclude that **the missingness of `descrpition` likely depends on the `sugar` column.**\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "We hypothesize that the missingness of `description` column does not depend on the `calorie` column, meaning that the distribution of calorie when `description` is missing and the distribution of calorie when `description` is not missing are alike, any difference is due to random chance. \n",
    "\n",
    "\n",
    "\n",
    "Set up:\n",
    "\n",
    "**Does the missingness of `description` depend on `minutes` column?**\n",
    "\n",
    "**Null Hypothesis**: the missingness of `description` does not depend on `minutes`. \n",
    "\n",
    "**Alternative Hypothesis**: the missingness of `description` does depend on `minutes`.\n",
    "\n",
    "**Significance Level**: 0.05\n",
    "\n",
    "\n",
    "\n",
    "Let's look at the distribution of `minutes`, conditional on the missingness of `description` column. \n",
    "\n",
    "<iframe src=\"figures/miss_dist_2.html\" width=800 height=600 frameBorder=0></iframe>\n",
    "\n",
    "Based on the plot above, the orange line illustrates the distribution of minutes when `description` is not missing, while the blue line illustrates the distribution of minutes when `description` is missing. Desipte both distribution have similar shape, they are centered in smiliar location. Using difference in mean may not effectively capture this difference in distribution. So we decide to use Kolmogorov-Smirnov (K-S) statistic as our test statistics for the permutation test.\n",
    "\n",
    "\n",
    "\n",
    "We shuffle the missingness of description 1000 times and get 1000 simulated K-S test statistics of `minutes` column during the permutation test. The permutation result is shown as below. \n",
    "\n",
    "<iframe src=\"figures/miss_perm_2.html\" width=800 height=600 frameBorder=0></iframe>\n",
    "\n",
    "From the plot above and result of the permutation test, the 0.05% significance level of the simulated statistics is 0.15 and our observed test statistics 0.1, which is smaller that the values of the significance level. The p-value is 0.435, which is greater than the significance level of 0.05. So we **fail to reject the null hypothesis.**\n",
    "\n",
    "Thus, we conclude that **the missingness of `descrpition` likely depends on the `minutes` column.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550fb6d5",
   "metadata": {},
   "source": [
    "## Hypothesis Testing\n",
    "\n",
    "The question we are going to explore and reasearch in this section is the following: **Do popular recipes (those with high review count) have a lower sugar level compared to less popular ones (those with low review count)** \n",
    "\n",
    "Recall that we define high review count threshold as having more than **10** reviews. We will conduct a permutationt test to see if the distribution of sugar level for popular recipes and the distribution of sugar level for not popular recipes are similar. \n",
    "\n",
    "Set up:\n",
    "\n",
    "**Null Hypothesis**: Receipes with a high review count do not have a lower sugar levels than those with low review count. Any observed differences in our samples are merely due to random chance.\n",
    "\n",
    "**Alternative Hypothesis**: Recipes with high review count indeed have a lower sugar level than those with a low review count. The observed difference observed in our samples cannot be explained by random chance alone.\n",
    "\n",
    "**Test Statistics**: Since our variable of interest is numerical and our test is one-tail test, signifying a directional alternative hypothesis, we will use difference in mean as our test statistics for the permutation test.\n",
    "\n",
    "**Significance Level**: 0.05\n",
    "\n",
    "<br>\n",
    "\n",
    "We created a new column called `is_popular` which is true if the recipe has reivew count more than 10, and false otherwise. \n",
    "\n",
    "This is a sample of the dataframe that we are going to perform permutation test on. \n",
    "\n",
    "|      |   sugar | is_popular   |\n",
    "|------|--------|-------------|\n",
    "|  1092 |      36 | False        |\n",
    "| 44285 |       4 | False        |\n",
    "| 60708 |      18 | False        |\n",
    "| 56138 |      48 | False        |\n",
    "|   628 |       0 | True         |\n",
    "\n",
    "<br>\n",
    "\n",
    "We shuffle the `is_popular` 1000 times and get 1000 simulated difference in mean test statistics of `sugar` column during the permutation test. The emperical distribution of permutation test result is shown as below. \n",
    "\n",
    "<iframe src=\"figures/hyp.html\" width=800 height=600 frameBorder=0></iframe>\n",
    "\n",
    "**P-value**: 0.0\n",
    "\n",
    "From looking at the graph above, we can see that our observed difference in mean, which is 14.58, is greater than the significance level of simulated difference in mean, which is 6.05, suggesting that the observed statistics in our sample are not merely coincidental. Furthermore, the p-value obtained from our permutation testing is 0.0, which falls below our significance level of 0.05. Therefore, we **reject our null hypothesis** and in favor of our alternative hypothesis: recipes with high review counts likely have a lower sugar level compared to those with low review counts.\n",
    "\n",
    "The result can be reasonable that people might like to give high rating to recipes with lower sugar content in food, and food with a lower sugar level is considered more healthy. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96813755",
   "metadata": {},
   "source": [
    "## Framing a Prediction Problem\n",
    "\n",
    "Recall from the introduction that we are interesting in the following problem:\n",
    "\n",
    "**What category does a recipe fall into based on its predicted popularity level and average rating score?**\n",
    "\n",
    "Category 1: Low review count, low average rating\n",
    "\n",
    "Category 2: Low review count, high average rating\n",
    "\n",
    "Category 3: High review count, low average rating\n",
    "\n",
    "Category 4: High review count, high average rating\n",
    "\n",
    "Note: we define the threshold for high and low review counts as **10** reviews count and the threshold for high and low average rating scores as **4.8**\n",
    "\n",
    "Specifically, we want to classify recipes into the above 4 categories based on all other information we have. \n",
    "\n",
    "<br>\n",
    "\n",
    "The prediction problem we are addressing is a **multi-class classification** problem since our goal is to classify recipes into one of four distinct categories. These class categories are determined by two other variables `rating` which categorizes recipes based on their average rating and `n_review` which categorizes recipes based on the number of reviews it has. Thus, recipes are classified as a combination of either having a low review count or a high review count, and either a low average rating or a high average rating, resulting in four unique class labels.\n",
    "\n",
    "<br>\n",
    "\n",
    "To generate features for our classification model, the variables we will be using are all other columns except `rating` and `n_review` since these two variables are used to create the class categories. \n",
    "\n",
    "The key metrics we will be using for evaluating our classifier model performance are **accuracy, precision, recall, and f1-score**:\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"figures/accuracy.png\" alt=\"accuracy formula\">\n",
    "\n",
    "- **Accuracy** represents the proportion of correctly classified instances among all observations. It tells us the overall performance of how the model classifies recipes into each category. However, accuracy does not tell the full story, especially when dealing with imbalanced data.\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"figures/precision.png\" alt=\"precision formula\">\n",
    "\n",
    "- **Precision** measures the proportion of the predicted positive instances that are correctly classified. It tells us how good the model is at avoiding false positive predictions. In our content, a false positive occurs when low-rating recipes are mistakenly classified into high-rating categories. A high precision minimizes the occurrence of such misclassification. \n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"figures/recall.png\" alt=\"recall formula\">\n",
    "\n",
    "- **Recall** measures the proportion of the actual positive instances that are correctly classified. It tells us how good the model is at identifying all the positive instances that are present, without missing too many of them. In our context, it reflects the model’s ability to correctly classify all high-rating recipes into the high-rating categories. A high recall maximizes the occurrence of the correct classification. \n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"figures/f1-score.png\" alt=\"f1-score formula\">\n",
    "\n",
    "- **F1-score**, being the harmonic mean of precision and recall, provides a balanced summary of the model’s predictive power, considering both predicted positive and actual positive. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee60a319",
   "metadata": {},
   "source": [
    "## Baseline Model: A Simple Approach\n",
    "\n",
    "We split our data to training and testing set by stratifing using the class label. The training set consititue 80% of our data while the testing has the reamining 20% of our data. We will use testing set to evalute the ability of our model to generalize to unseen data.:\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(recipe.drop(['class', 'rating', 'n_review'], axis=1), \n",
    "                                                        recipe['class'], test_size=0.2, stratify=recipe['class'])\n",
    "\n",
    "### Feature Engineering\n",
    "\n",
    "We perform the following feature engineering steps to transform our vairables before fitting into our model. We use RobustScaler instead of StandardScaler on most numerical column because there exist extreme values in those columns. RobustScaler use the median instead of the mean while scaling the data. \n",
    "\n",
    "<br>\n",
    "\n",
    "**Quantative Feature**:\n",
    "\n",
    "`minutes`, `protein`, `sodium`, `saturated_fat`, `total_fat`, `carbohydrates`\n",
    "- Type: quantative continuous\n",
    "- Feature Transformation: use RobustScaler to reduce the impact of outlier\n",
    "\n",
    "`n_steps`, `n_ingredients`\n",
    "- Type: quantative discrete\n",
    "- Feature Transformation: passthrough\n",
    "\n",
    "`time`\n",
    "- Type: quantative\n",
    "- Feature Transformation: extract year, month, and day from `submitted` timestamp column\n",
    "\n",
    "<br>\n",
    "\n",
    "**Categorical Feature**:\n",
    "\n",
    "`calories`, `sugar`\n",
    "- Type: quantative to nominal\n",
    "- Feature Transformation: cateogrize calories and sugar into 8 bins and do one-hot encoding\n",
    "\n",
    "`recipe_complexity`\n",
    "- Type: nominal\n",
    "- Feature Transformation: binarize `n_steps` and `n_ingredients` using the threshold of 10 to represent recipe complexity\n",
    "\n",
    "We choose to build features from the above columns as we believe these features might have some relationship for predicting recipe's popularity and average rating. \n",
    "\n",
    "### Baseline Model Building and Performance Evaluation\n",
    "\n",
    "For Baseline Model, we decide to use the Random Forest model for our classification problem. \n",
    "\n",
    "The main idea of Random Forest algorithm is to Fit n number of decision trees by using bagging and a random subset of features at each split. Predict by taking a vote from those n decision trees. It is the idea of Ensemble Learning.\n",
    "\n",
    "<br>\n",
    "\n",
    "Here is a pipeline of our baseline model, in which we transform our column first, then fit into RandomForestClassifier. \n",
    "\n",
    "    Pipeline(steps=[('col_trans',\n",
    "                     ColumnTransformer(transformers=[('outlier', RobustScaler(),\n",
    "                                                      ['minutes', 'protein',\n",
    "                                                       'sodium', 'saturated_fat',\n",
    "                                                       'total_fat',\n",
    "                                                       'carbohydrates']),\n",
    "                                                     ('pass', 'passthrough',\n",
    "                                                      ['n_steps', 'n_ingredients']),\n",
    "                                                     ('to_bin',\n",
    "                                                      Pipeline(steps=[('outlier',\n",
    "                                                                       RobustScaler()),\n",
    "                                                                      ('to_bins',\n",
    "                                                                       KBinsDiscretizer(n_bins=8))]),\n",
    "                                                      ['calories', 'sugar']),\n",
    "                                                     ('time',\n",
    "                                                      Pipeline(steps=[('time',\n",
    "                                                                       FunctionTransformer(func=<function <lambda> at 0x2b514b550>))]),\n",
    "                                                      ['submitted']),\n",
    "                                                     ('complexity',\n",
    "                                                      Pipeline(steps=[('complex',\n",
    "                                                                       FunctionTransformer(func=<function recipe_complexity at 0x2b5056ee0>))]),\n",
    "                                                      ['n_steps',\n",
    "                                                       'n_ingredients'])])),\n",
    "                    ('clf', RandomForestClassifier())])\n",
    "\n",
    "<br>\n",
    "\n",
    "After fitting our training data into the baseline model, we evalute our model using testing data. The confusion matrix below is the result of the prediction of testing data. \n",
    "\n",
    "<iframe src=\"figures/base_confus.html\" width=800 height=600 frameBorder=0></iframe>\n",
    "\n",
    "\n",
    "By looking at the confusion matrix, we can see that the baseline model misclassifies lots of recipe into class lable 1 and 2. In addition, it have difficult in accurately classifying recipes into class label 3 and 4, which correspond to high review counts. This is caused by the imbalanced nature of class label in our dataset. **Recipes with high review counts are relatively rare and constitute a minority group of data**, whereas recipes with low review count are more prevalent and make up a majority of the data. This is something we will address in building our final model. \n",
    "\n",
    "Result of counting all the class lable in our dataset, showing imbalanced data:\n",
    "\n",
    "    2    48979\n",
    "    1    28825\n",
    "    4     1834\n",
    "    3     1535\n",
    "    Name: class, dtype: int64\n",
    "\n",
    "<br>\n",
    "\n",
    "Let's look at the precision, recall, and f1-score of our baseline model for predicting unseen data. \n",
    "\n",
    "\n",
    "|            | precision | recall | f1-score | support |\n",
    "|------------|-----------|--------|----------|---------|\n",
    "|   1        |   0.42    |  0.13  |   0.20   |  5765   |\n",
    "|   2        |   0.61    |  0.91  |   0.73   |  9796   |\n",
    "|   3        |   0.00    |  0.00  |   0.00   |   307   |\n",
    "|   4        |   0.00    |  0.00  |   0.00   |   367   |\n",
    "|accuracy    |           |        |   0.59   | 16235   |\n",
    "|macro avg   |   0.26    |  0.26  |   0.23   | 16235   |\n",
    "|weighted avg|   0.52    |  0.59  |   0.51   | 16235   |\n",
    "\n",
    "We see that the model has 0 precision, recall, and f1-score for class label 3 and 4 while it has a high f1-score for class label 2 since most recipes belongs to class 2. Our model accuracy is 59%. Overall, the performane of our baseline model is not as good as we thought so since it has difficult in identifying popular recipes, which are something we are more interested in. We will improve our baseline model to correctly classify more popular recipes. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486c4f52",
   "metadata": {},
   "source": [
    "## Final Model: Balanced Random Forest & Binary Classification\n",
    "\n",
    "As usual, we will us the same training and testing data from baseline mode. \n",
    "\n",
    "#### Feature Engineering\n",
    "\n",
    "**The following features are from the baseline model**\n",
    "\n",
    "`minutes`, `protein`, `sodium`, `saturated_fat`, `total_fat`, `carbohydrates`\n",
    "- Type: quantative continuous\n",
    "- Feature Transformation: use RobustScaler to reduce the impact of outlier\n",
    "\n",
    "`n_steps`, `n_ingredients`\n",
    "- Type: quantative discrete\n",
    "- Feature Transformation: passthrough\n",
    "\n",
    "`time`\n",
    "- Type: quantative\n",
    "- Feature Transformation: extract year, month, and day from `submitted` timestamp column\n",
    "\n",
    "`recipe_complexity`\n",
    "- Type: nominal\n",
    "- Feature Transformation: binarize `n_steps` and `n_ingredients` to represent recipe complexity\n",
    "\n",
    "<br>\n",
    "\n",
    "**The following text features are added for the final model:**\n",
    "\n",
    "`description`\n",
    "- Type: text data\n",
    "- Feature Transformation: \n",
    " - Build a list of vocabulary from high Inverse Document Frequency(IDF) words from the `description` of **high reivew count recipes** (minority class label, class 3 and 4)\n",
    " - Vectorize the description text column using TF-IDF and the vocabulary from previous step\n",
    " - For each recipe, extract the top 5 highest TF-IDF values as the features\n",
    "\n",
    "`steps`\n",
    "- Type: text data\n",
    "- Feature Transformation: \n",
    " - Build a list of vocabulary from high Inverse Document Frequency(IDF) words from the `steps` of **high reivew count recipes** (minority class label, class 3 and 4)\n",
    " - Vectorize the steps text column using TF-IDF and the vocabulary from previous step\n",
    " - For each recipe, extract the top 5 highest TF-IDF values as the features \n",
    "\n",
    "<br>\n",
    "\n",
    "We believe that incorporating `description` and `steps` feature can improve our model's ability to identify recipes with high review counts since we built a vocabulary from those recipes and use TF-IDF to extract important text information. While there is a potential risk of data leakage since we use the class label to build feature, our approach to using the class label is indirect and we only use the training data to build vocabulary. We believe it is appropraite to use these text feature in our final model. \n",
    "\n",
    "<br>\n",
    "\n",
    "Simple sentiment analysis on `reviews` column:\n",
    "\n",
    "`reviews`\n",
    "- Type: text data\n",
    "- Feature Transformation:\n",
    " - Manaually create a list of sentiment words such as `good`, `loved`, `hated`, etc that are relevant for extracting sentiment information\n",
    " - For each word in sentiment word list, binarize `reviews` based on whether there are enough review comments that contain that particular word\n",
    " \n",
    "We think that adding this sentiment analysis on `reviews` feature can as well improve our model's ability to distinguish recipes between high average rating and low average rating by the simple logic that high rating recipes often have more positive words while low rating recipe often have more negative words. \n",
    "\n",
    "---\n",
    "\n",
    "### Final Model Building\n",
    "\n",
    "The approach we aim to use for our final model is to decompose our multi-class classification problem into two distinct binary classification problems. We will construct two separate classification models, one to determine whether a recipe has a high or low review count, and another to determine whether a recipe has a high or low average rating. Subsequently,  we will combine the outcomes of these two binary classifiers to generate the four different class labels corresponding to the result of our multi-class classification problem.\n",
    "\n",
    "We observed previously that there is a severe data imbalance issue in classifying recipes based on review count, where the low review count category represents the majority class, while the high review count category represents the minority class. To address this imbalance, the model we are going to use is the **BalancedRandomForest** algorithm from the imbalanced-learn library. Unlike the standard random forest implemented in sklearn, balanced random forest us bootstrapping to sample from the minority class and randomly selects the same number of samples with replacement from the majority class while constructing each decision tree. This approach will help our model’s ability to have a higher precision in classifying the minority class. \n",
    "\n",
    "A simple model construction of our idea:\n",
    "\n",
    "    class ClassificationTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "        def __init__(self, model_1, model_2):\n",
    "            self.model_1 = clone(model_1)\n",
    "            self.model_2 = clone(model_2)\n",
    "\n",
    "        def fit(self, X, Y):\n",
    "            self.model_2.fit(X, Y[1])\n",
    "            self.model_1.fit(X, Y[0])\n",
    "\n",
    "            return self\n",
    "\n",
    "        def predict(self, X):\n",
    "            y_1 = self.model_1.predict(X)\n",
    "            y_2 = self.model_2.predict(X)\n",
    "            \n",
    "            # combine the outcome of y_1 and y_2 \n",
    "            ...\n",
    "            \n",
    "            return y\n",
    "\n",
    "<br>\n",
    "\n",
    "Model pipeline: \n",
    "\n",
    "    Pipeline(steps=[('col_trans',\n",
    "                     ColumnTransformer(transformers=[('outlier', RobustScaler(),\n",
    "                                                      ['minutes', 'protein',\n",
    "                                                       'sodium', 'saturated_fat',\n",
    "                                                       'total_fat',\n",
    "                                                       'carbohydrates']),\n",
    "                                                     ('pass', 'passthrough',\n",
    "                                                      ['n_steps', 'n_ingredients']),\n",
    "                                                     ('time',\n",
    "                                                      Pipeline(steps=[('time',\n",
    "                                                                       FunctionTransformer(func=<function <lambda> at 0x2b514b550>))]),\n",
    "                                                      ['submitted']), ...])),\n",
    "                    ('clf',\n",
    "                     ClassificationTransformer(model_1=BalancedRandomForestClassifier(class_weight='balanced',\n",
    "                                                                                      criterion='entropy',\n",
    "                                                                                      max_depth=16,\n",
    "                                                                                      n_estimators=180),\n",
    "                                               model_2=RandomForestClassifier(class_weight='balanced',\n",
    "                                                                              criterion='entropy',\n",
    "                                                                              max_depth=19,\n",
    "                                                                              n_estimators=150)))])\n",
    "\n",
    "We use standard random forest to classify recipes based on average rating and use balanced random forest to classify recipes based on review counts, since it has more imbalanced data. \n",
    "\n",
    "---\n",
    "\n",
    "### Hyperparameter Tunning\n",
    "\n",
    "We manually iterative through a list of hyperparameter with stratified 5-fold trian-test split seperately for our two classifier models to find the best hyperparameter for model accuracy. We found the best `max_depth` hyperparameter to be 16 and 19 and the best `num_estimators` hyperparameter to be 180 and 150 for balanced ranfom forest and standard random forest, respectively, as seen in our pipeline above. \n",
    "\n",
    "---\n",
    "\n",
    "### Model Performance Evaluation\n",
    "\n",
    "After fitting our training data into the final model, we evalute our model using testing data. The confusion matrix below is the result of the prediction of testing data. \n",
    "\n",
    "<iframe src=\"figures/final_confus.html\" width=800 height=600 frameBorder=0></iframe>\n",
    "\n",
    "As seen in the confusion matrix, our final model has correctly classified a considerable amount of minority class label, class 3 and 4, which is overall an improvement to the baseline model. \n",
    "\n",
    "Let's look at the precision, recall, and f1-score of our final model for predicting unseen data.\n",
    "\n",
    "|            | precision | recall | f1-score | support |\n",
    "|------------|-----------|--------|----------|---------|\n",
    "|   1        |   0.59    |  0.47  |   0.53   |  5765   |\n",
    "|   2        |   0.74    |  0.73  |   0.73   |  9796   |\n",
    "|   3        |   0.20    |  0.54  |   0.29   |   307   |\n",
    "|   4        |   0.21    |  0.66  |   0.32   |   367   |\n",
    "|accuracy    |           |        |   0.63   | 16235   |\n",
    "|macro avg   |   0.44    |  0.60  |   0.47   | 16235   |\n",
    "|weighted avg|   0.67    |  0.63  |   0.64   | 16235   |\n",
    "\n",
    "\n",
    "Although the precision for class 3 and 4 are relatively low, their recall are high, which means that **our model is good at capturing all the high reiview count recipes, without missing too many of them, but at the same time, it makes too many false positive, misclassify low review count recipes into high review count categories.** The trade off is acceptaible. The accuracy of our final model is 63%, which is better than our baseline model. Overall, our final model is an improvement over the baseline model. \n",
    "\n",
    "<br>\n",
    "\n",
    "<iframe src=\"figures/final_feature.html\" width=800 height=600 frameBorder=0></iframe>\n",
    "\n",
    "We can gain insights into our model's feature importance by visualization. Feature importance highlights the extent to which our engineered feature contributes to help the model's classification decision. A higher importance score indicates that the feature plays a more significant role in classifying recipes. We can see from the plot that certain features have a very high importance to the model. Specifically, the `review` feature corresponds to features numbered from 21-50. Recall that we use a list of manually created sentiment words for simple sentiment analysis. The result of the visualization above indicates that certain sentiment words are particularly useful in helping the model to make accurate predictions. \n",
    "\n",
    "Below, we present the top 10 most useful sentiment words for feature engineering in our classification model:\n",
    "\n",
    "    1        great\n",
    "    2         good\n",
    "    3         very\n",
    "    4    delicious\n",
    "    5         made\n",
    "    6          but\n",
    "    7        loved\n",
    "    8      perfect\n",
    "    9        would\n",
    "    10    wonderful\n",
    "\n",
    "<br>\n",
    "\n",
    "Finally, we fit our final model using all availbale data and ship to production for fairness analysis. \n",
    "\n",
    "    final_model = pl_clf.fit(\n",
    "        recipe.drop(['class', 'rating', 'n_review'], axis=1),\n",
    "        [to_n_review(recipe['class']), to_rating(recipe['class'])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184f775a",
   "metadata": {},
   "source": [
    "## Fairness Analysis\n",
    "\n",
    "For fairness analysis, we are interested in this question: **“Are recipes with `vegetarian` tags more likely to be correctly classified as to the high average rating category by the model, compared to those without the `vegetarian` tags?”** Are our model fair in terms of precision?\n",
    "\n",
    "To evaluate fairness, we will compare the **precision** acorss two distinct groups. Specifically, we will compare the precision score for recipes with the 'vegetarian' tag against those without it. If the precision for recipes with the `vegetarian` tag is statistically significantly higher than the precision for recipes without it, it could potentially indicate a bias towards classifying `vegetarian` recipes as high average rating more frequently, even when they should not be classified as such. In our dataset, a high average rating corresponds to class categories 2 and 4. \n",
    "\n",
    "<br>\n",
    "\n",
    "Setup:\n",
    "\n",
    "**Null Hypothesis**: Our model is fair. Our classifier's precision is the same for recipes with and without `vegetarian` tag, and any differences are due to random chance.\n",
    "\n",
    "**Alternative Hypothesis**: Our model is not fair. Our classifier's precision is higher for recipes with `vegetarian` tag than those without, and any observed differences can not be explained by random chance alone.\n",
    "\n",
    "**Test statistic**: Difference in average precision of class 2 and 4 (without `vegetarian` tag - with `vegetarian` tag).\n",
    "\n",
    "**Significance level**: 0.05.\n",
    "\n",
    "<br>\n",
    "\n",
    "We fitted our final model with all available and created a new column `has_tags_vegetarian` that indicates if the recipes has `vegetarian` tag. \n",
    "\n",
    "We then shuffle the `has_tags_vegetarian` 1000 times and get 1000 simulated difference in average precision test statistics for recipes with and without `vegetarian` tag during the permutation test. The emperical distribution of permutation test result is shown as below. \n",
    "\n",
    "<iframe src=\"figures/fair.html\" width=800 height=600 frameBorder=0></iframe>\n",
    "\n",
    "From the graph above, we can see that the observed difference in precision falls below the significance level of 0.05, suggesting that the observed statistics in our sample are likely by random chance alone. The p-value we obtained from performing our permutation testing is approximately 0.33, which is greater than our significance level of 0.05. Therefore, we **fail to feject our null hypothesis** that the precision of our classifier is likely around the same for recipes with and without `vegetarian` tag, and any observed differences are due to random chance. Our model **achieves precision parity** across groups with and without `vegetarian`tag. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
